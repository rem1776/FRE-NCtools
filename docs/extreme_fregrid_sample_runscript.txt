# The NCTools app fregrid_parallel is the parallel version of fregrid, and
# it is particularly useful for processing with large grids. fregrid will
# generate this message when the remapping weights file is not available and
# grids are too large for it to process:
#
#   FATAL Error: The xgrid size is too large for resources.
#    nxgrid is greater than MAXXGRID/nthreads; increase MAXXGRID,
#    decrease nthreads, or increase number of MPI ranks.
#
# fregrid_parallel should instead be used if such an error is encountered. Like the
# serial version, it can be used to perform the remapping; but commonly it is used
# to perform only the compute intensive operations - the generation of the remapping
# weights file. The weights file is saved and in turn it is then used (and re-used)
# as an input file to the serial fregrid to quickly perform the remapping. However,
# even fregrid_parallel can generate the same error if run with insufficient
# computational resources. Below (after the dashed line) is an example runscript
# configured for running fregrid_parallel with a sufficiently large number of ranks
# and cores to avoid the fatal error for a common "extreme fregrid" case. This
# configuration runs in about 43 minutes on Gaea C5. The tail end of the runs output
# follows:
#
#> NOTE: done calculating index and weight for conservative interpolation
#> Memuse(MB) at After setup interp, min=4814.17, max=4881.2, avg=4834.05
#> Running time for get_input_grid, min=2.35723, max=4.71106, avg=4.4872
#> Running time for get_output_grid, min=0.000376, max=0.000725, avg=0.000484754
#> Running time for setup_interp, min=2517.68, max=2571.2, avg=2558.28
#> NOTE: Successfully running fregrid and the following files which
#>  store weight information are generated.
#> ****lg_remap_C3072_11520x5760.nc

--------------------------------------------------------------------------------
#!/bin/csh -f
#SBATCH -J Run_Script
#SBATCH --nodes=41
#SBATCH --time 4:00:00
#SBATCH --cluster=c5
#SBATCH --partition=batch
#SBATCH --qos=normal
#SBATCH --account=gfdl_f

source $MODULESHOME/init/tcsh
module load fre/bronx-20

set echo=on

# Break up the run so the first MPI-rank is on a node by itself to eventually allow for
# coalescing of the exchange grid-based remap file to the first rank for output
# The remaining mpi-ranks can share nodes and may need to be run on a reduced set
# to allow for memory pressure amongst the worker nodes

set nt1=1
set cpt1=64
set nt2=640
set cpt2=4

srun --ntasks=$nt1 --cpus-per-task=$cpt1 \
  fregrid_parallel --input_mosaic C3072_mosaic.nc --nlon 11520 --nlat 5760 \
  --remap_file lg_remap_C3072_11520x5760.nc --interp_method conserve_order1 --debug \
  : \
  --ntasks $nt2 --cpus-per-task=$cpt2 \
  fregrid_parallel --input_mosaic C3072_mosaic.nc --nlon 11520 --nlat 5760 \
  --remap_file lg_remap_C3072_11520x5760.nc --interp_method conserve_order1 --debug

